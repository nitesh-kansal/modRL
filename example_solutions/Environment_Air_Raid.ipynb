{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6831f82-a67d-4fde-95a5-0cd49b025b88",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/n0k03zp/Desktop/miniforge3/envs/pytorch_latest_gpu_gym_petting_zoo/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "We're Unable to find the game \"AirRaid\". Note: Gymnasium no longer distributes ROMs. If you own a license to use the necessary ROMs for research purposes you can download them via `pip install gymnasium[accept-rom-license]`. Otherwise, you should try importing \"AirRaid\" via the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \"AirRaid\" is unsupported. To check if this is the case try providing the environment variable `PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# defining the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexample_environment_classes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgym_env_class\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIGym\n\u001b[0;32m----> 3\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIGym\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mALE/AirRaid-v5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/RL/example_solutions/example_environment_classes/gym_env_class.py:9\u001b[0m, in \u001b[0;36mOpenAIGym.__init__\u001b[0;34m(self, envname, n_consecutive_states)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, envname, n_consecutive_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# create the environment\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# number of actions\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n",
      "File \u001b[0;32m~/Desktop/miniforge3/envs/pytorch_latest_gpu_gym_petting_zoo/lib/python3.10/site-packages/gymnasium/envs/registration.py:642\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m     render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 642\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43menv_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    645\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m apply_human_rendering\n\u001b[1;32m    647\u001b[0m     ):\n",
      "File \u001b[0;32m~/Desktop/miniforge3/envs/pytorch_latest_gpu_gym_petting_zoo/lib/python3.10/site-packages/shimmy/atari_env.py:171\u001b[0m, in \u001b[0;36mAtariEnv.__init__\u001b[0;34m(self, game, mode, difficulty, obs_type, frameskip, repeat_action_probability, full_action_space, max_num_frames_per_episode, render_mode)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39msetBool(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msound\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Seed + Load\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_action_space:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgetLegalActionSet()\n",
      "File \u001b[0;32m~/Desktop/miniforge3/envs/pytorch_latest_gpu_gym_petting_zoo/lib/python3.10/site-packages/shimmy/atari_env.py:216\u001b[0m, in \u001b[0;36mAtariEnv.seed\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39msetInt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m, seed2\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32))\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(roms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game):\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Error(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWe\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mre Unable to find the game \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Note: Gymnasium no longer distributes ROMs. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you own a license to use the necessary ROMs for research purposes you can download them \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvia `pip install gymnasium[accept-rom-license]`. Otherwise, you should try importing \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvia the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis unsupported. To check if this is the case try providing the environment variable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m     )\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mloadROM(\u001b[38;5;28mgetattr\u001b[39m(roms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game))\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mError\u001b[0m: We're Unable to find the game \"AirRaid\". Note: Gymnasium no longer distributes ROMs. If you own a license to use the necessary ROMs for research purposes you can download them via `pip install gymnasium[accept-rom-license]`. Otherwise, you should try importing \"AirRaid\" via the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \"AirRaid\" is unsupported. To check if this is the case try providing the environment variable `PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "from example_environment_classes.gym_env_class import OpenAIGym\n",
    "env = OpenAIGym(\"ALE/AirRaid-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3a839-9fb0-496d-86c2-0779250f51e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QNetworkPixelAtari(env.n_consecutive_states*3, env.action_size, 1234, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "346e5c5c-b210-4ba8-b967-98f1997a239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqnLoop import dqn_training_loop, watch_trained_agent\n",
    "params = {}\n",
    "params[\"n_episodes\"] = 600         # maximum number of training episodes\n",
    "params[\"max_t\"] = 1000             # maximum number of timesteps per episode\n",
    "params[\"policy_type\"] = \"e_greedy\"\n",
    "\n",
    "# Parameters useful in case of epsilon greedy action selection\n",
    "params[\"eps_start\"] = 1.           # starting value of epsilon, for epsilon-greedy action selection\n",
    "params[\"eps_end\"] = 0.01           # minimum value of epsilon\n",
    "params[\"eps_decay\"] = 0.997        # multiplicative factor (per episode) for decreasing epsilon\n",
    "\n",
    "# Parameters useful in case of annealed softmax action selection\n",
    "params[\"t_start\"] = 5.           # starting value of temperature, for epsilon-greedy action selection\n",
    "params[\"t_end\"] = 0.005           # minimum value of temperature\n",
    "params[\"t_decay\"] = 0.95        # multiplicative factor (per episode) for decreasing temperature\n",
    "\n",
    "# Parameters for controlling dependence of sampling probability on TD-error in Prioritized Experience Replay\n",
    "params[\"alpha0\"] = 0.0             # Initial value of alpha parameter in Prioritized Experience Replay \n",
    "params[\"alphaf\"] = 0.0            # Final value of alpha parameter in Prioritized Experience Replay \n",
    "params[\"nsteps_alpha\"] = 500       # Number of episodes in which to linearly change alpha from alpha0 to alphaf\n",
    "\n",
    "# Parameters for controlling Importance Sampling weight (ISw) in Prioritized Experience Replay\n",
    "params[\"beta0\"] = 0.             # Final value of beta parameter in Prioritized Experience Replay\n",
    "params[\"betaf\"] = 0.                # Initial value of beta parameter in Prioritized Experience Replay\n",
    "params[\"nsteps_beta\"] = 500        # Number of episodes in which to linearly change beta from beta0 to betaf\n",
    "\n",
    "#DQN Update Parameters\n",
    "params[\"LR\"] = 3e-3                # Learning Rate for update of DQN weights\n",
    "params[\"BUFFER_SIZE\"] = 1000000     # Size of the Replay Buffer\n",
    "params[\"TAU\"] = 5e-3               # Fraction of primary network weights to be copied over to the target network after each parameter update step \n",
    "                                        # θ_target = τ*θ_primary + (1 - τ)*θ_target\n",
    "params[\"BATCH_SIZE\"] = 64          # Size of the sample to be selected at random from the Replay Buffer at each update step\n",
    "params[\"UPDATE_EVERY\"] = 2         # Number of actions (or transitions to be recorded) to be taken before making any update to DQN weights\n",
    "params[\"SAMPLE_FREQ\"] = 2          # Number of batch sampling and DQN weight update steps to be carried out during the update step\n",
    "params[\"GAMMA\"] = 0.95              # Discount Factor\n",
    "params[\"IS_DDQN\"] = False          # Whether to enable the Double DQN improvement or continue with basic DQN\n",
    "params[\"MEMORY_TYPE\"] = \"normal\"   # Whether to go with prioritized memory buffer or uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd464d-a49d-4c55-b0ab-b236b15db1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 545.25\tAverage Episode Len : 542.315050505051\n",
      "Episode 200\tAverage Score: 506.50\tAverage Episode Len : 509.73525125628144\n",
      "Episode 235\tAverage Score: 494.25\tAverage Episode Len : 502.71063829787235"
     ]
    }
   ],
   "source": [
    "output_scores, agent, optimal_selection_p  = dqn_training_loop(env, model, 3000., 100, params)\n",
    "score_plot(output_scores, optimal_selection_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "913d690b-34c5-46c7-8be4-d1e7fe9cebfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2.        ,  3.        ],\n",
       "        [21.33333333, 20.33333333],\n",
       "        [11.33333333,  1.33333333]]),\n",
       " array([0, 0, 0, 2, 2, 2, 1, 1, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[1,2],[2,3],[3,4],[10,1],[11,2],[13,1],[25,13],[15,23],[24,25]]\n",
    "X = np.array(X)\n",
    "k = 3\n",
    "kmean_loop(X, k, tol = 0.001, num_iteration = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea321a0-b616-4861-95e9-1191c7964d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_latest_gpu_gym_petting_zoo",
   "language": "python",
   "name": "pytorch_latest_gpu_gym_petting_zoo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
