{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6831f82-a67d-4fde-95a5-0cd49b025b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/n0k03zp/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A.L.E: Arcade Learning Environment (version 0.8.0+919230b)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 6\n",
      "images have shape : (250, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "from example_environments.General_openAI_gym_env import OpenAIGym\n",
    "from example_q_models.qmodel_AirRaid import QNetworkPixelAtari\n",
    "env = OpenAIGym(\"ALE/AirRaid-v5\")\n",
    "model = QNetworkPixelAtari(env.n_consecutive_states*3, env.action_size, 1234, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346e5c5c-b210-4ba8-b967-98f1997a239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqnLoop import dqn_training_loop, watch_trained_agent\n",
    "params = {}\n",
    "params[\"n_episodes\"] = 600         # maximum number of training episodes\n",
    "params[\"max_t\"] = 1000             # maximum number of timesteps per episode\n",
    "params[\"policy_type\"] = \"e_greedy\"\n",
    "\n",
    "# Parameters useful in case of epsilon greedy action selection\n",
    "params[\"eps_start\"] = 1.           # starting value of epsilon, for epsilon-greedy action selection\n",
    "params[\"eps_end\"] = 0.01           # minimum value of epsilon\n",
    "params[\"eps_decay\"] = 0.995        # multiplicative factor (per episode) for decreasing epsilon\n",
    "\n",
    "# Parameters useful in case of annealed softmax action selection\n",
    "params[\"t_start\"] = 5.           # starting value of temperature, for epsilon-greedy action selection\n",
    "params[\"t_end\"] = 0.005           # minimum value of temperature\n",
    "params[\"t_decay\"] = 0.95        # multiplicative factor (per episode) for decreasing temperature\n",
    "\n",
    "# Parameters for controlling dependence of sampling probability on TD-error in Prioritized Experience Replay\n",
    "params[\"alpha0\"] = 0.0             # Initial value of alpha parameter in Prioritized Experience Replay \n",
    "params[\"alphaf\"] = 0.0            # Final value of alpha parameter in Prioritized Experience Replay \n",
    "params[\"nsteps_alpha\"] = 500       # Number of episodes in which to linearly change alpha from alpha0 to alphaf\n",
    "\n",
    "# Parameters for controlling Importance Sampling weight (ISw) in Prioritized Experience Replay\n",
    "params[\"beta0\"] = 0.             # Final value of beta parameter in Prioritized Experience Replay\n",
    "params[\"betaf\"] = 0.                # Initial value of beta parameter in Prioritized Experience Replay\n",
    "params[\"nsteps_beta\"] = 500        # Number of episodes in which to linearly change beta from beta0 to betaf\n",
    "\n",
    "#DQN Update Parameters\n",
    "params[\"LR\"] = 1e-3                # Learning Rate for update of DQN weights\n",
    "params[\"BUFFER_SIZE\"] = 1000000     # Size of the Replay Buffer\n",
    "params[\"TAU\"] = 1e-2               # Fraction of primary network weights to be copied over to the target network after each parameter update step \n",
    "                                        # θ_target = τ*θ_primary + (1 - τ)*θ_target\n",
    "params[\"BATCH_SIZE\"] = 64          # Size of the sample to be selected at random from the Replay Buffer at each update step\n",
    "params[\"UPDATE_EVERY\"] = 4         # Number of actions (or transitions to be recorded) to be taken before making any update to DQN weights\n",
    "params[\"SAMPLE_FREQ\"] = 1          # Number of batch sampling and DQN weight update steps to be carried out during the update step\n",
    "params[\"GAMMA\"] = 0.9              # Discount Factor\n",
    "params[\"IS_DDQN\"] = False          # Whether to enable the Double DQN improvement or continue with basic DQN\n",
    "params[\"MEMORY_TYPE\"] = \"normal\"   # Whether to go with prioritized memory buffer or uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38dd464d-a49d-4c55-b0ab-b236b15db1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 551.00\tAverage Episode Len : 543.921616161617\n",
      "Episode 200\tAverage Score: 418.00\tAverage Episode Len : 485.16547236180905\n",
      "Episode 300\tAverage Score: 288.25\tAverage Episode Len : 446.27333333333333\n",
      "Episode 400\tAverage Score: 256.50\tAverage Episode Len : 410.60581954887227\n",
      "Episode 500\tAverage Score: 236.50\tAverage Episode Len : 395.92219238476955\n",
      "Episode 600\tAverage Score: 223.75\tAverage Episode Len : 379.78833333333336\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'score_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m output_scores, agent, optimal_selection_p  \u001b[38;5;241m=\u001b[39m dqn_training_loop(env, model, \u001b[38;5;241m3000.\u001b[39m, \u001b[38;5;241m100\u001b[39m, params)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mscore_plot\u001b[49m(output_scores, optimal_selection_p)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score_plot' is not defined"
     ]
    }
   ],
   "source": [
    "output_scores, agent, optimal_selection_p  = dqn_training_loop(env, model, 3000., 100, params)\n",
    "score_plot(output_scores, optimal_selection_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188f120-1779-4b6f-9723-e7e01cdf996d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
